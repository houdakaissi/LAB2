# Deep Learning Lab Report
#Objective   ![Uploading Capture d'écran 2024-03-20 170202.png…]()
The main objective of this lab is to gain familiarity with the PyTorch library and build various neural architectures for computer vision tasks. Specifically, we aim to implement CNN, Faster R-CNN, and Vision Transformer (ViT) models for classifying the MNIST dataset.
Part 1: CNN Classifier
![Uploading Capture d'écran 2024-03-20 170202.png…]()
1. CNN Architecture
Implemented a CNN architecture using PyTorch to classify the MNIST dataset.
Defined convolutional, pooling, and fully connected layers.
Specified hyperparameters such as kernels, padding, stride, and optimizers.
Ran the model in GPU mode for improved performance.
2. Faster R-CNN
Implemented the Faster R-CNN architecture for MNIST dataset classification.
3. Model Comparison
Compared the performance of the CNN and Faster R-CNN models using metrics such as accuracy, F1 score, loss, and training time.
4. Fine-tuning with VGG16 and AlexNet
Retrained pre-trained models (VGG16 and AlexNet) on the MNIST dataset.
Compared the performance of fine-tuned models with CNN and Faster R-CNN.
Part 2: Vision Transformer (ViT)
1. ViT Architecture
Followed a tutorial to establish a Vision Transformer (ViT) model architecture from scratch using PyTorch.
Performed a classification task on the MNIST dataset using ViT.
2. Model Comparison
Interpreted the results obtained from the ViT model and compared them with the results from the CNN and Faster R-CNN models.
