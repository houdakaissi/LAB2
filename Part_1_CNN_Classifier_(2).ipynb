{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Establish a CNN Architecture (Based on Pytorch Library) to classify MINST Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "fHdLltmxc3tO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N4APAcIkW8P"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PSX4j55OkeV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q64jM1rnki-c",
        "outputId": "ca7d9bfe-9753-4bbc-b7a4-278d05cac449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 144978785.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 26681870.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 47360360.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12042053.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "AUf3Pp6Ykmfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RRnAnq3kpv9",
        "outputId": "a232a9a9-d345-4e47-9c5a-e6fb6e0ec399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 0.688\n",
            "[1,   200] loss: 0.199\n",
            "[1,   300] loss: 0.128\n",
            "[1,   400] loss: 0.106\n",
            "[1,   500] loss: 0.089\n",
            "[1,   600] loss: 0.081\n",
            "[1,   700] loss: 0.071\n",
            "[1,   800] loss: 0.060\n",
            "[1,   900] loss: 0.066\n",
            "[2,   100] loss: 0.053\n",
            "[2,   200] loss: 0.052\n",
            "[2,   300] loss: 0.047\n",
            "[2,   400] loss: 0.050\n",
            "[2,   500] loss: 0.051\n",
            "[2,   600] loss: 0.037\n",
            "[2,   700] loss: 0.041\n",
            "[2,   800] loss: 0.047\n",
            "[2,   900] loss: 0.037\n",
            "[3,   100] loss: 0.037\n",
            "[3,   200] loss: 0.031\n",
            "[3,   300] loss: 0.028\n",
            "[3,   400] loss: 0.029\n",
            "[3,   500] loss: 0.033\n",
            "[3,   600] loss: 0.032\n",
            "[3,   700] loss: 0.026\n",
            "[3,   800] loss: 0.037\n",
            "[3,   900] loss: 0.028\n",
            "[4,   100] loss: 0.020\n",
            "[4,   200] loss: 0.022\n",
            "[4,   300] loss: 0.024\n",
            "[4,   400] loss: 0.025\n",
            "[4,   500] loss: 0.028\n",
            "[4,   600] loss: 0.023\n",
            "[4,   700] loss: 0.022\n",
            "[4,   800] loss: 0.029\n",
            "[4,   900] loss: 0.024\n",
            "[5,   100] loss: 0.013\n",
            "[5,   200] loss: 0.017\n",
            "[5,   300] loss: 0.014\n",
            "[5,   400] loss: 0.013\n",
            "[5,   500] loss: 0.021\n",
            "[5,   600] loss: 0.029\n",
            "[5,   700] loss: 0.015\n",
            "[5,   800] loss: 0.015\n",
            "[5,   900] loss: 0.019\n",
            "[6,   100] loss: 0.013\n",
            "[6,   200] loss: 0.016\n",
            "[6,   300] loss: 0.009\n",
            "[6,   400] loss: 0.014\n",
            "[6,   500] loss: 0.017\n",
            "[6,   600] loss: 0.017\n",
            "[6,   700] loss: 0.015\n",
            "[6,   800] loss: 0.011\n",
            "[6,   900] loss: 0.018\n",
            "[7,   100] loss: 0.007\n",
            "[7,   200] loss: 0.007\n",
            "[7,   300] loss: 0.012\n",
            "[7,   400] loss: 0.007\n",
            "[7,   500] loss: 0.018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LElRzwYPkwDN",
        "outputId": "9e4d70b7-fe93-4032-a0fa-ca13482dc589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 99 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Do the same thing with Faster R-CNN"
      ],
      "metadata": {
        "id": "WEmFH6qndHa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import struct\n",
        "from array import array\n",
        "class FasterRCNNNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FasterRCNNNet, self).__init__()\n",
        "        self.faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "        in_features = self.faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.faster_rcnn_model.roi_heads.box_predictor = nn.Linear(in_features, num_classes)\n",
        "    def load_pretrained_weights(self, model_path):\n",
        "        state_dict = torch.load(model_path)\n",
        "        mapping_dict = {\n",
        "            \"conv1.weight\": \"backbone.body.conv1.weight\",\n",
        "            \"bn1.weight\": \"backbone.body.bn1.weight\",\n",
        "        }\n",
        "        adjusted_state_dict = {}\n",
        "        for key, value in state_dict.items():\n",
        "            if key in mapping_dict:\n",
        "                adjusted_key = mapping_dict[key]\n",
        "                adjusted_state_dict[adjusted_key] = value\n",
        "        self.faster_rcnn_model.load_state_dict(adjusted_state_dict, strict=False)\n",
        "    def forward(self, images, targets=None):\n",
        "        if self.training and targets is not None:\n",
        "            targets = [{k: v for k, v in target.items()} for target in targets]\n",
        "            loss_dict = self.faster_rcnn_model(images, targets)\n",
        "            return sum(loss for loss in loss_dict.values())\n",
        "        else:\n",
        "            return self.faster_rcnn_model(images)"
      ],
      "metadata": {
        "id": "J8-6NPDGrKcW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRCNNPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(FastRCNNPredictor, self).__init__()\n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.cls_score(x)"
      ],
      "metadata": {
        "id": "AYvREKujblPQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Convert list of pixels to PIL image\n",
        "        image = PIL.Image.fromarray(np.array(image))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        height, width = image.shape[-2:]\n",
        "        box = torch.tensor([0, 0, width, height], dtype=torch.float32)\n",
        "\n",
        "        sample = {\n",
        "            \"images\": image,\n",
        "            \"boxes\": box,\n",
        "            \"labels\": torch.tensor(label, dtype=torch.int64)\n",
        "        }\n",
        "        return sample"
      ],
      "metadata": {
        "id": "OIRUIk2Jbr46"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath, transform=None):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "        self.transform = transform\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img\n",
        "        return images, labels\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        train_dataset = MnistDataset(x_train, y_train, transform=self.transform)\n",
        "        test_dataset = MnistDataset(x_test, y_test, transform=self.transform)\n",
        "        return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "j-gU0KYjb1Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_dataloader = MnistDataloader(\n",
        "    '/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
        "    '/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
        "    '/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
        "    '/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "train_dataset, test_dataset = mnist_dataloader.load_data()\n",
        "mnist_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "mnist_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "R9WLpQqjb4oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "faster_rcnn_net = FasterRCNNNet(num_classes)\n",
        "model_path = \"/kaggle/input/cocodataset/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
        "faster_rcnn_net.load_pretrained_weights(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "faster_rcnn_net.to(device)\n",
        "optimizer = optim.Adam(faster_rcnn_net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "id": "5ldXiP-eb5VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"images\"].to(device)\n",
        "        boxes = batch[\"boxes\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        targets = [{\"boxes\": boxes, \"labels\": labels}]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, targets)\n",
        "        loss = sum(outputs.values())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "MU9dNK3Wb8Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total_samples\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "On-gVeJPb_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_classes = 10\n",
        "faster_rcnn_net = FasterRCNNNet(num_classes)\n",
        "model_path = \"/kaggle/input/cocodataset/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
        "faster_rcnn_net.load_pretrained_weights(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "faster_rcnn_net.to(device)\n",
        "optimizer = optim.Adam(faster_rcnn_net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.to(device)"
      ],
      "metadata": {
        "id": "l1CuNUc7cB5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using retrained models (VGG16 and AlexNet) fine tune your model to the new dataSet,4."
      ],
      "metadata": {
        "id": "QAq-zhCodNXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "NV9VQq1PcJk7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = np.array(self.images[index], dtype=np.uint8).reshape(28, 28, 1)\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9eIc7iNgcXKy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath, transform=None):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "        self.transform = transform\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img\n",
        "        return images, labels\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        train_dataset = MnistDataset(x_train, y_train, transform=self.transform)\n",
        "        test_dataset = MnistDataset(x_test, y_test, transform=self.transform)\n",
        "        return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "2JFTQJcZcb6-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xTey6bfbcfBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mnist_dataloader = MnistDataloader('/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte',\n",
        "                                   '/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte',\n",
        "                                   '/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte',\n",
        "                                   '/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte',\n",
        "                                   transform=transform)\n",
        "\n",
        "train_dataset, test_dataset = mnist_dataloader.load_data()\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "gLogfX5Hcfl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model_cnn = CNN().to(device)"
      ],
      "metadata": {
        "id": "PC6Ku0DIcij8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16OneChannel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16OneChannel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model_vgg16_one_channel = VGG16OneChannel().to(device)"
      ],
      "metadata": {
        "id": "78keXmUtcjTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class alexnetChannel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(alexnetChannel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model_alexnet = alexnetChannel().to(device)"
      ],
      "metadata": {
        "id": "v-6i-eAUcm7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(model, train_loader, test_loader, num_epochs=5, learning_rate=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "5imu5e2ycpyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kltEJv6xct6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}